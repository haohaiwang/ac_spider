import os
import re
import time
import random
import hashlib
import requests
from collections import defaultdict
from urllib.parse import urlparse, parse_qs, urljoin, urlencode, urlunparse

from PIL import Image
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# ====== é…ç½® ======
BASE_LIST_URL = "https://www.mideahk.com/%E7%A9%BA%E6%B0%A3%E6%B5%81%E9%80%9A/%E5%86%B7%E6%B0%A3%E6%A9%9F"
SAVE_ROOT = "midea_ac_images"
PAGE_LOAD_TIMEOUT = 30
WAIT = 2.0  # é¡µé¢æ¸²æŸ“é¢å¤–ç­‰å¾…ï¼ˆç§’ï¼‰
HEADLESS = True  # è‹¥è¦çœ‹æµè§ˆå™¨è¿‡ç¨‹ï¼Œæ”¹ä¸º False
MAX_PAGES = 80   # éå¸¸è§„å¾ªç¯ä¿æŠ¤
PAGE_PARAM = "page"  # ç«™ç‚¹ä½¿ç”¨çš„åˆ†é¡µå‚æ•°å
MAX_ALLOWED_PAGE = 3  # â€”â€” å…³é”®é™åˆ¶ï¼šæœ€å¤šåˆ°ç¬¬ 3 é¡µ â€”â€” #

# ====== é€šç”¨ ======
def ensure_dir(p):
    os.makedirs(p, exist_ok=True)

def safe_name(s: str) -> str:
    s = re.sub(r'[\\/:*?"<>|\s]+', "_", s).strip("_")
    return s or f"item_{int(time.time()*1000)}"

def build_driver():
    options = Options()
    if HEADLESS:
        options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=zh-CN,zh,en-US,en")
    # é™å™ªä¸ç¨³å®š
    options.add_argument("--log-level=3")
    options.add_argument("--disable-notifications")
    options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument("--remote-allow-origins=*")

    driver_path = ChromeDriverManager().install()
    service = Service(driver_path)
    driver = webdriver.Chrome(service=service, options=options)
    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
    return driver

def scroll_to_bottom(driver, pause=0.6, max_rounds=20):
    last_height = driver.execute_script("return document.body.scrollHeight || document.documentElement.scrollHeight;")
    rounds = 0
    while rounds < max_rounds:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(pause)
        new_height = driver.execute_script("return document.body.scrollHeight || document.documentElement.scrollHeight;")
        if new_height == last_height:
            break
        last_height = new_height
        rounds += 1

def pick_from_srcset(srcset: str) -> str:
    if not srcset:
        return ""
    # é€‰ç¬¬ä¸€ä¸ª URL
    return srcset.split(",")[0].strip().split(" ")[0].strip('"').strip("'")

def extract_img_urls_from_img_elements(driver, base_url: str):
    """ä»é¡µé¢ä¸Š class=img-center çš„ <img> å…ƒç´ æå–å›¾ç‰‡ URLï¼Œå…¼å®¹ src/srcset/data-*ï¼Œ
    ä¹Ÿå…¼å®¹ <picture><source srcset> ä»¥åŠ background-imageã€‚"""
    urls = set()

    # 1) <img class="img-center"> ä¸ 2) çˆ¶å®¹å™¨ .img-center ä¸‹çš„ img
    imgs = driver.find_elements(By.CSS_SELECTOR, 'img.img-center, .img-center img')

    # 3) <picture> é‡Œ source[srcset]
    sources = driver.find_elements(By.CSS_SELECTOR, 'picture source[srcset]')

    # 4) èƒŒæ™¯å›¾å†™åœ¨ style ä¸Š
    bg_nodes = driver.find_elements(By.CSS_SELECTOR, '.img-center, .img-center *')

    for img in imgs:
        src = (
            img.get_attribute("src")
            or img.get_attribute("data-src")
            or img.get_attribute("data-original")
        )
        if not src:
            srcset = img.get_attribute("srcset")
            if srcset:
                src = pick_from_srcset(srcset)
        if not src:
            outer = img.get_attribute("outerHTML") or ""
            m = re.search(r'(?:src|data-src|data-original)\s*=\s*["\']([^"\']+)', outer, re.I)
            if m:
                src = m.group(1)
        if not src:
            continue
        src = src.split("?")[0]
        full = urljoin(base_url, src)
        if full.startswith("http"):
            urls.add(full)

    for s in sources:
        srcset = s.get_attribute("srcset")
        u = pick_from_srcset(srcset)
        if u:
            urls.add(urljoin(base_url, u.split("?")[0]))

    for n in bg_nodes:
        style = (n.get_attribute("style") or "")
        m = re.search(r'background(?:-image)?\s*:\s*url\(([^)]+)\)', style, re.I)
        if m:
            u = m.group(1).strip('"').strip("'")
            if u and not u.startswith('data:'):
                urls.add(urljoin(base_url, u.split("?")[0]))

    return urls

def product_id_from_url(url: str) -> str:
    parsed = urlparse(url)
    qs = parse_qs(parsed.query)
    pid = qs.get("product_id", [None])[0]
    if pid:
        return f"product_{pid}"
    tail = safe_name(os.path.basename(parsed.path) or "unknown")
    return f"product_{tail}"

def download(url: str, save_path: str, referer: str = None):
    for attempt in range(3):
        try:
            headers = {
                "User-Agent": (
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/120.0.0.0 Safari/537.36"
                ),
            }
            if referer:
                headers["Referer"] = referer
            with requests.get(url, headers=headers, stream=True, timeout=25) as r:
                r.raise_for_status()
                with open(save_path, "wb") as f:
                    for chunk in r.iter_content(8192):
                        if chunk:
                            f.write(chunk)
            print(f"  âœ… saved: {save_path}")
            return
        except Exception as e:
            print(f"  âš ï¸ retry({attempt+1}) : {url} -> {e}")
            time.sleep(1.2 + random.random())
    print(f"  âŒ fail : {url}")

def unique_save_path(folder: str, img_url: str) -> str:
    fname = os.path.basename(img_url.split("?")[0]) or f"{int(time.time()*1000)}.jpg"
    fname = safe_name(fname)
    p = os.path.join(folder, fname)
    if os.path.exists(p):
        base, ext = os.path.splitext(fname)
        p = os.path.join(folder, f"{base}_{int(time.time()*1000)}{ext or '.jpg'}")
    return p

# ====== URL/é¡µç å·¥å…· ======
def normalize_http_url(candidate: str, base_url: str):
    """æŠŠ candidate æ ‡å‡†åŒ–ä¸ºç»å¯¹ URLï¼Œä¸”å¿…é¡»æ˜¯ http/https åè®®ï¼›å¦åˆ™è¿”å› Noneã€‚"""
    if not candidate:
        return None
    c = candidate.strip()
    # è¿‡æ»¤æ— æ•ˆåè®®
    if c in ("#", "javascript:void(0)", "void(0)"):
        return None
    lc = c.lower()
    if lc.startswith("javascript:") or lc.startswith("data:") or lc.startswith("mailto:"):
        return None
    # åˆå¹¶ç›¸å¯¹/ç»å¯¹
    c = urljoin(base_url, c)
    parsed = urlparse(c)
    if parsed.scheme not in ("http", "https"):
        return None
    return c

def get_page_index(url: str) -> int:
    """è¿”å› URL ä¸­ page å‚æ•°ï¼ˆé»˜è®¤ 1ï¼‰ï¼Œéæ³•åˆ™æŒ‰ 1 å¤„ç†"""
    try:
        qs = parse_qs(urlparse(url).query)
        return int(qs.get(PAGE_PARAM, ["1"])[0] or "1")
    except Exception:
        return 1

def _increment_page_url(url: str, step: int = 1) -> str:
    """å…œåº•ï¼šåŸºäº ?page= é€’å¢ç”Ÿæˆä¸‹ä¸€é¡µ URLï¼ˆä¿æŒå…¶ä»– query ä¸å˜ï¼‰"""
    parsed = urlparse(url)
    qs = parse_qs(parsed.query)
    cur = int(qs.get(PAGE_PARAM, [1])[0] or 1)
    nxt = cur + step
    qs[PAGE_PARAM] = [str(nxt)]
    flat = {}
    for k, v in qs.items():
        flat[k] = v if isinstance(v, list) and len(v) != 1 else (v[0] if isinstance(v, list) else v)
    query = urlencode(flat, doseq=True)
    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, query, parsed.fragment))

# ====== æ–‡ä»¶å¤¹å»é‡ ======
def remove_empty_dirs(root_dir: str, remove_root: bool = False):
    """
    é€’å½’åˆ é™¤ root_dir ä¸‹çš„æ‰€æœ‰ç©ºæ–‡ä»¶å¤¹ã€‚
    - remove_root=False æ—¶ï¼Œä¿ç•™æ ¹ç›®å½•æœ¬èº«ï¼ˆå³ä½¿å®ƒæ˜¯ç©ºçš„ï¼‰ã€‚
    """
    removed = 0
    for dirpath, dirnames, filenames in os.walk(root_dir, topdown=False):
        # è‹¥ç›®å½•ä¸ºç©ºï¼ˆä¸åŒ…å«ä»»ä½•æ–‡ä»¶/å­ç›®å½•ï¼‰
        try:
            if not os.listdir(dirpath):
                # æ ¹ç›®å½•æ˜¯å¦ä¿ç•™
                if (dirpath == root_dir) and (not remove_root):
                    continue
                os.rmdir(dirpath)
                removed += 1
                print(f"ğŸ—‘ï¸ removed empty dir: {dirpath}")
        except Exception as e:
            print(f"âš ï¸ remove dir failed: {dirpath} -> {e}")
    print(f"Empty directories removed: {removed}")

# ====== åˆ—è¡¨é¡µæŠ“å– + åˆ†é¡µ ======
def collect_product_links_from_current_page(driver):
    links = set()
    # 1) ç›´è¿: å¸¦ product_id å‚æ•°
    for a in driver.find_elements(By.CSS_SELECTOR, 'a[href*="product_id="]'):
        href = normalize_http_url(a.get_attribute("href"), driver.current_url)
        if href:
            links.add(href)
    # 2) slug å½¢å¼ï¼Œå¦‚ /product/xxx æˆ– /products/xxx
    for a in driver.find_elements(By.CSS_SELECTOR, 'a[href*="/product/"] , a[href*="/products/"]'):
        href = normalize_http_url(a.get_attribute("href"), driver.current_url)
        if href:
            links.add(href)
    # 3) å¡ç‰‡å¸¸è§ç»“æ„ï¼šåŒ…å«äº§å“å¡ class çš„é“¾æ¥
    for a in driver.find_elements(By.CSS_SELECTOR, 'a.product, a.product-card, .product a, .product-item a, .products a'):
        href = normalize_http_url(a.get_attribute("href"), driver.current_url)
        if href:
            links.add(href)
    return links

def find_next_page_url(driver, base_url):
    """å¢å¼ºç‰ˆï¼šæ”¯æŒ <i class="next">ï¼Œå¹¶æŠŠé¡µç ä¸¥æ ¼é™åˆ¶åœ¨ 1~3 ä¹‹å†…"""
    def _limit_and_return(href: str):
        h = normalize_http_url(href, base_url)
        if not h:
            return None
        if get_page_index(h) > MAX_ALLOWED_PAGE:
            return None
        return h

    # 1) rel="next"
    el = driver.find_elements(By.CSS_SELECTOR, 'a[rel="next"]')
    if el:
        out = _limit_and_return(el[0].get_attribute('href'))
        if out:
            return out

    # 2) å¸¸è§ .next é“¾æ¥
    el = driver.find_elements(By.CSS_SELECTOR, 'a.next, li.next a, .pagination-next a')
    if el:
        out = _limit_and_return(el[0].get_attribute('href'))
        if out:
            return out

    # 3) ç‰¹æ®Šï¼š<i class="next">ï¼Œæ‰¾æœ€è¿‘ç¥–å…ˆ <a> çš„ hrefï¼›æ²¡æœ‰å°±å°è¯•ç‚¹å‡»åè¯»å– current_url
    try:
        i_next = driver.find_element(By.CSS_SELECTOR, 'i.next')
        try:
            ancestor_a = i_next.find_element(By.XPATH, 'ancestor::a[1]')
        except Exception:
            ancestor_a = None

        if ancestor_a:
            out = _limit_and_return(ancestor_a.get_attribute('href'))
            if out:
                return out

        # æ²¡æœ‰å¯ç”¨ hrefï¼Œå°è¯•ç‚¹å‡»ä»¥è§¦å‘è·³è½¬ï¼ˆè‹¥æ˜¯å‰ç«¯è·¯ç”±ï¼‰
        try:
            driver.execute_script("arguments[0].click();", ancestor_a or i_next)
            time.sleep(WAIT)
            cur_after_click = driver.current_url
            out = _limit_and_return(cur_after_click)
            if out and out != base_url:
                return out
        except Exception:
            pass
    except Exception:
        pass

    # 4) æ•°å­—åˆ†é¡µï¼šå½“å‰é¡¹çš„ä¸‹ä¸€ä¸ªå…„å¼Ÿ
    current = None
    for li in driver.find_elements(By.CSS_SELECTOR, 'li.page-numbers, .pagination li, .pager li'):
        cls = (li.get_attribute('class') or '')
        if 'current' in cls or 'active' in cls:
            current = li
            break
    if current:
        try:
            nxt = current.find_element(By.XPATH, 'following-sibling::li[1]//a')
            out = _limit_and_return(nxt.get_attribute('href'))
            if out:
                return out
        except Exception:
            pass

    # 5) å…œåº•ï¼šæŒ‰ ?page= é€’å¢ï¼ˆä»ç„¶å— 1~3 é™åˆ¶ï¼‰
    try:
        guess = _increment_page_url(driver.current_url or base_url, step=1)
        out = _limit_and_return(guess)
        if out and out != base_url:
            return out
    except Exception:
        pass

    return None

def collect_product_links(driver, start_url: str):
    seen_pages = set()
    product_links = set()
    cur = normalize_http_url(start_url, start_url)
    pages = 0

    while cur and cur not in seen_pages and pages < MAX_PAGES:
        # â€”â€” è®¿é—®å‰ä¿é™©ï¼šåªæŠ“ 1~3 é¡µ â€”â€” #
        if get_page_index(cur) > MAX_ALLOWED_PAGE:
            print(f"âš ï¸ è¶…è¿‡ç¬¬ {MAX_ALLOWED_PAGE} é¡µï¼Œåœæ­¢ï¼š{cur}")
            break

        print(f"ğŸ“„ åˆ—è¡¨é¡µï¼š{cur}")
        seen_pages.add(cur)
        driver.get(cur)
        WebDriverWait(driver, PAGE_LOAD_TIMEOUT).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(WAIT)
        scroll_to_bottom(driver)
        # æŠ“æœ¬é¡µäº§å“é“¾æ¥
        product_links |= collect_product_links_from_current_page(driver)

        # æ‰¾ä¸‹ä¸€é¡µï¼ˆå…¼å®¹ <i class="next">ï¼‰
        nxt = find_next_page_url(driver, cur)
        # â€”â€” ä¸‹ä¸€é¡µä¿é™©ï¼šåªæŠ“ 1~3 é¡µ â€”â€” #
        if nxt and (nxt not in seen_pages) and get_page_index(nxt) <= MAX_ALLOWED_PAGE:
            cur = nxt
            pages += 1
            time.sleep(0.8 + random.random())
            continue
        break

    # â€”â€” å¼ºåŒ–è¡¥æŠ“ï¼šå›ºå®šè¡¥ 1/2/3 é¡µï¼Œé¿å…æ¼æŠ“ï¼ˆä¸ä¼šè¶Šç•Œï¼‰ â€”â€” #
    try:
        parsed = urlparse(start_url)
        base_no_query = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, "", parsed.fragment))
        candidates = [
            base_no_query,  # ç¬¬ 1 é¡µ
            f"{base_no_query}?{PAGE_PARAM}=2",
            f"{base_no_query}?{PAGE_PARAM}=3",
        ]
        for c in candidates:
            c_norm = normalize_http_url(c, start_url)
            if c_norm and c_norm not in seen_pages and get_page_index(c_norm) <= MAX_ALLOWED_PAGE:
                print(f"ğŸ“„ï¼ˆè¡¥æŠ“ï¼‰åˆ—è¡¨é¡µï¼š{c_norm}")
                seen_pages.add(c_norm)
                driver.get(c_norm)
                WebDriverWait(driver, PAGE_LOAD_TIMEOUT).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                time.sleep(WAIT)
                scroll_to_bottom(driver)
                product_links |= collect_product_links_from_current_page(driver)
                time.sleep(0.5 + random.random())
    except Exception:
        pass

    print(f"å…±å‘ç°äº§å“é“¾æ¥ï¼š{len(product_links)}ï¼Œè·¨è¶Šåˆ—è¡¨é¡µï¼š{len(seen_pages)}")
    return sorted(product_links)

# ====== è¯¦æƒ…é¡µæŠ“å›¾ ======
def crawl_one_product(driver, url: str, root: str):
    print(f"\nğŸ§­ æ‰“å¼€äº§å“ï¼š{url}")
    try:
        driver.get(url)
    except Exception as e:
        print(f"âŒ æ— æ³•æ‰“å¼€ï¼š{e}")
        return 0

    WebDriverWait(driver, PAGE_LOAD_TIMEOUT).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    time.sleep(WAIT)

    # æ‡’åŠ è½½æ»šåŠ¨
    scroll_to_bottom(driver, pause=0.4, max_rounds=8)

    img_urls = extract_img_urls_from_img_elements(driver, url)
    if not img_urls:
        print("âš ï¸ æœªæ‰¾åˆ° img-center çš„å›¾ç‰‡")
        return 0

    pid = product_id_from_url(url)
    folder = os.path.join(root, pid)
    ensure_dir(folder)

    saved = 0
    for u in sorted(img_urls):
        sp = unique_save_path(folder, u)
        download(u, sp, referer=url)
        saved += 1
    return saved

# ====== å»é‡å·¥å…·ï¼ˆç²¾ç¡® + å¯é€‰æ„ŸçŸ¥ï¼‰ ======
def _md5_of_file(path, chunk=1024 * 1024):
    h = hashlib.md5()
    with open(path, "rb") as f:
        while True:
            b = f.read(chunk)
            if not b:
                break
            h.update(b)
    return h.hexdigest()

def _ahash(img: Image.Image, hash_size=8) -> int:
    """å¹³å‡å“ˆå¸Œï¼ˆaHashï¼‰ï¼Œè¿”å› 64bit æ•´æ•°ã€‚"""
    small = img.convert("L").resize((hash_size, hash_size), Image.BILINEAR)
    pixels = list(small.getdata())
    avg = sum(pixels) / len(pixels)
    bits = 0
    for p in pixels:
        bits = (bits << 1) | (1 if p >= avg else 0)
    return bits

def _hamming_distance(x: int, y: int) -> int:
    return (x ^ y).bit_count()

def deduplicate_images(root_dir: str,
                       do_perceptual: bool = False,
                       ahash_threshold: int = 5,
                       dry_run: bool = False):
    """
    å¯¹ root_dir ä¸‹æ‰€æœ‰å›¾ç‰‡å»é‡ã€‚
    - å…ˆæŒ‰ MD5 åšâ€œå®Œå…¨é‡å¤â€å»é‡ï¼›
    - do_perceptual=True æ—¶ï¼Œå†ç”¨ aHash åšâ€œè¿‘ä¼¼é‡å¤â€å»é‡ï¼ˆæ±‰æ˜è·ç¦» <= ahash_thresholdï¼‰ã€‚
      å»ºè®®é˜ˆå€¼ 4~8ï¼Œé»˜è®¤ 5ï¼ˆåä¿å®ˆï¼‰ã€‚
    - dry_run=True æ—¶åªæ‰“å°ä¸åˆ é™¤ã€‚
    """
    print("\nğŸ§¹ Start deduplication...")

    # 1) æ”¶é›†æ‰€æœ‰å›¾ç‰‡è·¯å¾„
    exts = {".jpg", ".jpeg", ".png", ".webp", ".bmp", ".gif", ".tif", ".tiff"}
    all_imgs = []
    for base, _, files in os.walk(root_dir):
        for fn in files:
            ext = os.path.splitext(fn)[1].lower()
            if ext in exts:
                all_imgs.append(os.path.join(base, fn))

    print(f"Found {len(all_imgs)} images to check.")

    # 2) ç²¾ç¡®å»é‡ï¼ˆMD5ï¼‰
    md5_map = {}
    exact_dups = []
    for p in all_imgs:
        try:
            d = _md5_of_file(p)
        except Exception as e:
            print(f"  âš ï¸ hash fail: {p} -> {e}")
            continue
        if d in md5_map:
            exact_dups.append(p)
        else:
            md5_map[d] = p

    removed_exact = 0
    for p in exact_dups:
        print(f"  ğŸ—‘ï¸ exact dup: {p}")
        if not dry_run:
            try:
                os.remove(p)
                removed_exact += 1
            except Exception as e:
                print(f"    âš ï¸ remove fail: {p} -> {e}")

    print(f"Exact duplicates removed: {removed_exact}")

    if not do_perceptual:
        print("Perceptual (aHash) dedup skipped.")
        print("âœ… Deduplication done.\n")
        return

    # 3) è¿‘ä¼¼å»é‡ï¼ˆaHashï¼‰
    # é‡æ–°åˆ—å‡ºå‰©ä½™å›¾ç‰‡
    remain_imgs = []
    for base, _, files in os.walk(root_dir):
        for fn in files:
            ext = os.path.splitext(fn)[1].lower()
            if ext in exts:
                remain_imgs.append(os.path.join(base, fn))

    # å»ºç«‹ç²—åˆ†æ¡¶ï¼šæŒ‰å®½é«˜æ¯”ä¸å¤§å°åˆ†æ®µï¼Œå‡å°‘æ¯”è¾ƒæ¬¡æ•°
    size_buckets = defaultdict(list)
    for p in remain_imgs:
        try:
            sz = os.path.getsize(p)
            with Image.open(p) as im:
                w, h = im.size
            ratio_key = round(w / h, 2) if h else 0
            bin_key = (int(sz / 50_000), ratio_key)  # 50KB ä¸€æ®µ
            size_buckets[bin_key].append(p)
        except Exception:
            size_buckets[("misc", 0)].append(p)

    removed_perc = 0
    seen_hashes = []  # [(hash_int, path)]
    for _, paths in size_buckets.items():
        for p in paths:
            try:
                with Image.open(p) as im:
                    h_int = _ahash(im, hash_size=8)
            except Exception as e:
                print(f"  âš ï¸ aHash fail: {p} -> {e}")
                continue

            # ä¸å·²ä¿ç•™çš„å›¾æ¯”å¯¹
            is_dup = False
            for h_keep, p_keep in seen_hashes:
                dist = _hamming_distance(h_int, h_keep)
                if dist <= ahash_threshold:
                    print(f"  ğŸ—‘ï¸ near-dup ({dist}): {p}  ~  {p_keep}")
                    if not dry_run:
                        try:
                            os.remove(p)
                            removed_perc += 1
                        except Exception as e:
                            print(f"    âš ï¸ remove fail: {p} -> {e}")
                    is_dup = True
                    break
            if not is_dup:
                seen_hashes.append((h_int, p))

    print(f"Perceptual near-duplicates removed: {removed_perc}")
    print("âœ… Deduplication done.\n")

# ====== ä¸»æµç¨‹ ======
def main():
    ensure_dir(SAVE_ROOT)
    driver = build_driver()

    try:
        product_links = collect_product_links(driver, BASE_LIST_URL)
        total_imgs = 0
        for link in product_links:
            total_imgs += crawl_one_product(driver, link, SAVE_ROOT)
            time.sleep(0.2 + random.random())
        print(f"\nğŸ‰ å®Œæˆã€‚å…±ä¸‹è½½å›¾ç‰‡ï¼š{total_imgs} å¼ ã€‚ä¿å­˜ç›®å½•ï¼š{SAVE_ROOT}")
    finally:
        driver.quit()

    # â€”â€” ä¸‹è½½å®Œæˆååšå»é‡ â€”â€” #
    # 1) åªåšç²¾ç¡®å»é‡ï¼ˆå®‰å…¨ã€é›¶è¯¯åˆ ï¼‰
    deduplicate_images(SAVE_ROOT, do_perceptual=False)

    # â€”â€” å»é‡åæ¸…ç†ç©ºæ–‡ä»¶å¤¹ â€”â€” #
    remove_empty_dirs(SAVE_ROOT, remove_root=False)  # å¦‚éœ€è¿æ ¹ç›®å½•ä¹Ÿåˆ ï¼Œè®¾ä¸º True


    # 2) å¦‚éœ€è¿›ä¸€æ­¥å»ç›¸ä¼¼å›¾ï¼Œæ‰“å¼€ä¸‹é¢è¿™ä¸€è¡Œï¼ˆé»˜è®¤é˜ˆå€¼ 5ï¼Œå¯åœ¨ 4~8 ä¹‹é—´è°ƒæ•´ï¼‰
    # deduplicate_images(SAVE_ROOT, do_perceptual=True, ahash_threshold=5)

if __name__ == "__main__":
    main()
