# midea_products_scraper.py
# ç›®æ ‡ï¼šæŠ“å– https://www.mideahk.com/ç©ºæ°£æµé€š/å†·æ°£æ©Ÿ åˆ—è¡¨ç¬¬ 1~3 é¡µçš„æ‰€æœ‰äº§å“é“¾æ¥ï¼Œ
#       æ‰“å¼€è¯¦æƒ…é¡µåä¸‹è½½ class="img-center" çš„å›¾ç‰‡ï¼ˆæ”¯æŒ src/srcset/data-*ï¼‰ï¼Œ
#       å¹¶æŒ‰äº§å“ slug åˆ†ç›®å½•ä¿å­˜ã€‚

import os
import re
import time
import requests
from urllib.parse import urlparse
from typing import List, Set

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# ====== é…ç½® ======
BASE_LIST_URL = "https://www.mideahk.com/%E7%A9%BA%E6%B0%A3%E6%B5%81%E9%80%9A/%E5%86%B7%E6%B0%A3%E6%A9%9F"
SAVE_ROOT = "midea_ac_images"
# åˆ†é¡µï¼ˆæœ€å°æ”¹åŠ¨ï¼šåªè¦è°ƒè¿™é‡Œå°±å¥½ï¼‰
LIST_PAGES = [
    BASE_LIST_URL,
    f"{BASE_LIST_URL}?page=2",
    f"{BASE_LIST_URL}?page=3",
]

PAGE_LOAD_TIMEOUT = 35          # é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆç§’ï¼‰
WAIT = 2.0                      # é¡µé¢æ¸²æŸ“é¢å¤–ç­‰å¾…ï¼ˆç§’ï¼‰
SCROLL_STEPS = 8                # ä¸‹æ‹‰æ¬¡æ•°ï¼Œè§¦å‘æ‡’åŠ è½½
SCROLL_PAUSE = 0.8              # æ¯æ¬¡ä¸‹æ‹‰åçš„ç­‰å¾…
HEADLESS = True                 # è°ƒè¯•éœ€è¦çœ‹æµè§ˆå™¨è¿‡ç¨‹å¯è®¾ä¸º False
REQUEST_TIMEOUT = 20            # å›¾ç‰‡ä¸‹è½½è¶…æ—¶ï¼ˆç§’ï¼‰

# ====== é€šç”¨ ======
def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def safe_name(s: str) -> str:
    s = re.sub(r'[\\/:*?"<>|]', "_", s)
    return s.strip()[:100]

def build_driver():
    chrome_options = Options()
    if HEADLESS:
        chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1440,1000")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
    return driver

def js_ready(driver):
    try:
        return driver.execute_script("return document.readyState") == "complete"
    except Exception:
        return False

def try_accept_cookies(driver):
    # å¸¸è§ Cookie æŒ‰é’® id / æ–‡æ¡ˆ å…œåº•ç‚¹å‡»ï¼ˆå¿½ç•¥å¼‚å¸¸ï¼‰
    candidates = [
        (By.ID, "onetrust-accept-btn-handler"),
        (By.CSS_SELECTOR, "button[aria-label='Accept All']"),
        (By.XPATH, "//button[contains(., 'Accept') or contains(., 'åŒæ„') or contains(., 'æ¥å—')]"),
    ]
    for how, selector in candidates:
        try:
            btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((how, selector)))
            btn.click()
            time.sleep(0.5)
            break
        except Exception:
            pass

def smooth_scroll(driver, steps=SCROLL_STEPS, pause=SCROLL_PAUSE):
    last_h = 0
    for _ in range(steps):
        driver.execute_script("window.scrollBy(0, document.body.scrollHeight/3);")
        time.sleep(pause)
        try:
            new_h = driver.execute_script("return document.body.scrollHeight")
        except Exception:
            new_h = last_h
        if new_h == last_h:
            # å†è¡¥ä¸€æ¬¡åˆ°æœ€åº•
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(pause)
        last_h = new_h

# ====== åˆ—è¡¨æŠ“å– ======
def collect_product_links(driver, list_url: str) -> List[str]:
    print(f"ğŸ‘‰ æ‰“å¼€åˆ—è¡¨é¡µ: {list_url}")
    driver.get(list_url)

    # ç­‰å¾…æ–‡æ¡£ ready & å¤„ç† Cookie
    WebDriverWait(driver, 20).until(lambda d: js_ready(d))
    time.sleep(WAIT)
    try_accept_cookies(driver)

    # åˆ†æ®µä¸‹æ‹‰ï¼Œè§¦å‘æ‡’åŠ è½½
    smooth_scroll(driver)

    # å¤šé€‰æ‹©å™¨å…œåº•ï¼šä¸åŒæ¨¡æ¿ä¸‹äº§å“å¡ç‰‡ç»“æ„å¯èƒ½ä¸åŒ
    selector_candidates = [
        "div.proBox a",
        "div.product-thumb a",
        "div.product-layout a",
        "div.product-block a",
        "ul.products li a",
        "a.product_img",
        # ç›´æ¥ç”¨é“¾æ¥ç‰¹å¾ï¼ˆåˆ†ç±»è·¯å¾„å¯èƒ½ç¼–ç /æœªç¼–ç ï¼‰
        "a[href*='%E5%86%B7%E6%B0%A3%E6%A9%9F/']",
        "a[href*='/å†·æ°£æ©Ÿ/']",
    ]

    anchors = []
    for css in selector_candidates:
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, css))
            )
            found = driver.find_elements(By.CSS_SELECTOR, css)
            if found:
                anchors = found
                break
        except Exception:
            continue

    links: Set[str] = set()

    # ä¼˜å…ˆä»é€‰åˆ°çš„å…ƒç´ é‡Œæå–
    for a in anchors:
        href = a.get_attribute("href")
        if not href:
            continue
        if is_ac_product_url(href):
            links.add(href)

    # å¦‚æœ CSS æ²¡é€‰åˆ°æˆ–æ•°é‡å¤ªå°‘ï¼Œç”¨æ­£åˆ™ä»æ•´é¡µæºç å…œåº•
    if len(links) < 6:  # é˜ˆå€¼å¯è°ƒ
        html = driver.page_source or ""
        # åŒæ—¶åŒ¹é…ç¼–ç /æœªç¼–ç ä¸¤ç§è·¯å¾„ï¼›è¿‡æ»¤æ˜æ˜¾çš„åˆ—è¡¨/æœç´¢é¡µ
        pattern = re.compile(
            r'https?://(?:www\.)?mideahk\.com/[^\s"\'<>]*?(?:%E5%86%B7%E6%B0%A3%E6%A9%9F|å†·æ°£æ©Ÿ)/[^\s"\'<>/]+/?',
            flags=re.IGNORECASE
        )
        for m in pattern.findall(html):
            if is_ac_product_url(m):
                links.add(m)

    print(f"  æ”¶é›†åˆ° {len(links)} ä¸ªäº§å“é“¾æ¥")
    return list(links)

def is_ac_product_url(url: str) -> bool:
    # ç²—ç•¥æ’é™¤åˆ†ç±»é¡µã€åˆ†é¡µã€é”šç‚¹ç­‰ï¼Œåªä¿ç•™åƒ /å†·æ°£æ©Ÿ/<slug> çš„è¯¦æƒ…é¡µ
    if "page=" in url:
        return False
    path = urlparse(url).path.strip("/")
    parts = path.split("/")
    return (len(parts) >= 2) and (("å†·æ°£æ©Ÿ" in parts[0]) or ("%E5%86%B7%E6%B0%A3%E6%A9%9F" in parts[0]))

# ====== è¯¦æƒ…æŠ“å›¾ ======
def download_image(url, save_path):
    try:
        r = requests.get(url, timeout=REQUEST_TIMEOUT)
        r.raise_for_status()
        with open(save_path, "wb") as f:
            f.write(r.content)
        return True
    except Exception as e:
        print(f"    âŒ ä¸‹è½½å¤±è´¥ {url}: {e}")
        return False

def crawl_one_product(driver, product_url, save_root):
    print(f"\n=== æŠ“å–äº§å“: {product_url} ===")
    driver.get(product_url)
    WebDriverWait(driver, 20).until(lambda d: js_ready(d))
    time.sleep(WAIT)

    # ç›®å½•åç”¨æœ€åä¸€çº§ slug
    product_id = product_url.rstrip("/").split("/")[-1]
    product_dir = os.path.join(save_root, safe_name(product_id))
    ensure_dir(product_dir)

    # å¯èƒ½æœ‰æ‡’åŠ è½½ï¼Œå…ˆæ»šåŠ¨ä¸€é
    smooth_scroll(driver, steps=5, pause=0.6)

    imgs = driver.find_elements(By.CSS_SELECTOR, "img.img-center")
    # å…œåº•ï¼šæœ‰äº›è¯¦æƒ…é¡µå¯èƒ½ä¸æ˜¯è¿™ä¸ªç±»å
    if not imgs:
        imgs = driver.find_elements(By.CSS_SELECTOR, "img")

    seen = set()
    count = 0
    for img in imgs:
        for attr in ["src", "srcset", "data-src", "data-original"]:
            link = img.get_attribute(attr)
            if not link:
                continue
            for u in re.split(r",\s*", link):
                u = u.strip().split(" ")[0]
                if not u or u in seen:
                    continue
                if u.startswith("//"):
                    u = "https:" + u
                seen.add(u)
                fname = safe_name(os.path.basename(urlparse(u).path) or f"{len(seen)}.jpg")
                save_path = os.path.join(product_dir, fname)
                if download_image(u, save_path):
                    count += 1
    print(f"  âœ… ä¸‹è½½ {count} å¼ å›¾ç‰‡ -> {product_dir}")
    return count

# ====== ä¸»é€»è¾‘ ======
def main():
    ensure_dir(SAVE_ROOT)
    driver = build_driver()
    try:
        # æ”¶é›†ä¸‰é¡µæ‰€æœ‰äº§å“é“¾æ¥ï¼ˆå»é‡ï¼‰
        links_all: Set[str] = set()
        for list_url in LIST_PAGES:
            try:
                links_all.update(collect_product_links(driver, list_url))
            except Exception as e:
                print(f"âš ï¸ åˆ—è¡¨é¡µå¤±è´¥ {list_url}: {e}")

        product_links = sorted(links_all)
        print(f"\næ€»è®¡äº§å“é“¾æ¥ï¼š{len(product_links)}")

        total_imgs = 0
        for link in product_links:
            try:
                total_imgs += crawl_one_product(driver, link, SAVE_ROOT)
            except Exception as e:
                print(f"âš ï¸ äº§å“å¤±è´¥ {link}: {e}")
        print(f"\nğŸ‰ å®Œæˆã€‚å…±ä¸‹è½½å›¾ç‰‡ï¼š{total_imgs} å¼ ã€‚ä¿å­˜ç›®å½•ï¼š{SAVE_ROOT}")
    finally:
        try:
            driver.quit()
        except Exception:
            pass

if __name__ == "__main__":
    main()
